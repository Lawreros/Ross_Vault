name : Adam [[optimizer]]
tags : 
backlinks : 
source : https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/

###### Content:
The Adam optimization algorithm is an extension to [[stochastic gradient descent]] that is popular for deep learning applications in [[computer vision]] and [[natural language processing]].

###### Properties:
- Adam is a replacement optimization algorithm for [[stochastic gradient descent]] for training [[deep neural network]]s
- Adam combines the best properties of the AdaGrad and RMSProp algorithms to provide an optimization algorithm that can handle [[sparse gradient]]s on noisy problems.
- Adam is relatively easy to configure where the default configuration parameters do well on most problems

###### Additional Thoughts:
